import os
import yaml  # PyYAML (pip install PyYAML)
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm import tqdm
import open_clip
# --- [ìˆ˜ì • 1] AMP(Automatic Mixed Precision) ëª¨ë“ˆ ì„í¬íŠ¸ ---
from torch.cuda.amp import autocast, GradScaler 

# --- 1. ëª¨ë¸ ë° ë°ì´í„°ì…‹ ì„í¬íŠ¸ ---
try:
    # ì‹¤ì œ ëª¨ë¸ ë° ëª¨ë“ˆ ì„í¬íŠ¸
    from model.dpr_net import DPR_Net
    from model.dpr_modules import DprLLM # (DPRV2Lossê°€ LLM í† í¬ë‚˜ì´ì € í•„ìš”)
    from data.dataset import DPRDataset
except ImportError as e:
    print(f"Error: 'model' ë˜ëŠ” 'data' íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("G:\\DPR-Net\\model\\__init__.py ë˜ëŠ” G:\\DPR-Net\\data\\__init__.py íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

# -----------------------------------------------
#  â‘¦ V2 Training Loss (L_total)
#  (ì´ í´ë˜ìŠ¤ëŠ” ìˆ˜ì •í•  í•„ìš” ì—†ìŠµë‹ˆë‹¤. autocastê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.)
# -----------------------------------------------
class DPRV2Loss(nn.Module):
    """
    DPR-Net V2ì˜ â‘¦ë²ˆ ì „ì²´ ì†ì‹¤ í•¨ìˆ˜
    L_total = L_img + Î»1 * L_film + Î»2 * L_consistency
    """
    def __init__(self, 
                 lambda_film=0.1, 
                 lambda_consistency=0.01,
                 clip_model_name="ViT-L-14",
                 clip_pretrained="laion2b_s32b_b82k",
                 llm_embed_dim=4096
                 ):
        super().__init__()
        print(f"Initializing DPRV2Loss (Î»_film={lambda_film}, Î»_consistency={lambda_consistency})")
        
        self.lambda_film = lambda_film
        self.lambda_consistency = lambda_consistency

        # --- L_img (ë³µì› ì†ì‹¤) ---
        self.l1_loss = nn.L1Loss()
        
        # --- L_consistency (ì¼ê´€ì„± ì†ì‹¤) ---
        self.mse_loss = nn.MSELoss()
        
        print("  Loading CLIP Text Encoder for L_consistency...")
        self.clip_text_model, _, _ = open_clip.create_model_and_transforms(
            clip_model_name, pretrained=clip_pretrained
        )
        self.clip_tokenizer = open_clip.get_tokenizer(clip_model_name)
        
        for param in self.clip_text_model.parameters():
            param.requires_grad = False
        print("  CLIP Text Encoder frozen.")
        
        try:
            clip_text_embed_dim = self.clip_text_model.text_projection.shape[1]
        except AttributeError:
            clip_text_embed_dim = self.clip_text_model.text_projection.out_features
            
        print(f"  Inferred CLIP Text embed dim: {clip_text_embed_dim}")
            
        self.consistency_projector = nn.Linear(llm_embed_dim, clip_text_embed_dim) 

    def calculate_film_loss(self, hidden_state):
        num_tokens_to_align = 8 
        if hidden_state.shape[1] < num_tokens_to_align:
            return 0 
            
        control_tokens = hidden_state[:, :num_tokens_to_align, :]
        tokens_norm = F.normalize(control_tokens, p=2, dim=2)
        cosine_sim_matrix = torch.bmm(tokens_norm, tokens_norm.transpose(1, 2))
        eye = torch.eye(num_tokens_to_align, device=cosine_sim_matrix.device).expand_as(cosine_sim_matrix)
        off_diagonal_sim = cosine_sim_matrix * (1 - eye)
        loss_film = torch.mean(off_diagonal_sim.pow(2))
        return loss_film

    def calculate_consistency_loss(self, hidden_state, logits, llm_tokenizer, device):
        llm_global_hidden = hidden_state[:, 0, :] 
        projected_hidden = self.consistency_projector(llm_global_hidden)
        
        generated_tokens = torch.argmax(logits.detach(), dim=-1)
        generated_text_list = llm_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        clip_text_tokens = self.clip_tokenizer(generated_text_list).to(device)
        
        with torch.no_grad(): 
            clip_text_embed = self.clip_text_model.encode_text(clip_text_tokens)
            
        target_text_embed = clip_text_embed.float().detach()
        loss_consistency = self.mse_loss(projected_hidden, target_text_embed)
        return loss_consistency

    def forward(self, outputs, target_img, llm_tokenizer, device):
        l_img = self.l1_loss(outputs['img_restored'], target_img)
        
        l_film_val = 0
        if self.lambda_film > 0:
            l_film = self.calculate_film_loss(outputs['hidden_state'])
            l_film_val = l_film.item()
        else:
            l_film = 0.0

        l_consistency_val = 0
        if self.lambda_consistency > 0:
            l_consistency = self.calculate_consistency_loss(
                outputs['hidden_state'], 
                outputs['logits'], 
                llm_tokenizer,
                device
            )
            l_consistency_val = l_consistency.item()
        else:
            l_consistency = 0.0

        total_loss = l_img + \
                     self.lambda_film * l_film + \
                     self.lambda_consistency * l_consistency
                     
        loss_dict = {
            'total': total_loss.item(),
            'img': l_img.item(),
            'film': l_film_val,
            'consistency': l_consistency_val
        }
        
        return total_loss, loss_dict


# -----------------------------------------------
#  ğŸš€ ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (train.py)
# -----------------------------------------------
def main(config_path):
    # --- 1. ì„¤ì • ë¡œë“œ (YAML) ---
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"Error: Config file not found at {config_path}")
        return
        
    cfg_paths = config['paths']
    cfg_train = config['train']
    cfg_loss = config['loss']
    cfg_model_vet = config['model']['vetnet']
    cfg_model_clip = config['model']['clip']
    cfg_model_llm = config['model']['llm']
    
    # --- 2. ì„¤ì • ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    os.makedirs(cfg_paths['checkpoint_dir'], exist_ok=True)

    # --- 3. ë°ì´í„° ë¡œë” (DPRDataset ì‚¬ìš©) ---
    print(f"Loading REAL dataset from: {cfg_paths['data_dir']}")
    train_dataset = DPRDataset(
        data_dir=cfg_paths['data_dir'],
        mode='train',
        patch_size=cfg_train['patch_size'],
        hybrid_prob=0.5 
    )
    
    num_workers = 4 # (ì‹œìŠ¤í…œì— ë§ê²Œ ì¡°ì ˆ, 0ë„ ê°€ëŠ¥)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=cfg_train['batch_size'], 
        shuffle=True, 
        num_workers=num_workers,
        pin_memory=True,
        # --- [ìˆ˜ì • 2] ë°ì´í„° ë¡œë” ìµœì í™” ---
        persistent_workers=(num_workers > 0),
        prefetch_factor=2 # (ì„ íƒ ì‚¬í•­, ë°ì´í„° ì¤€ë¹„ ì†ë„ í–¥ìƒ)
    )
    
    # --- 4. ëª¨ë¸ ì´ˆê¸°í™” (DPR_Net ì‚¬ìš©) ---
    print("Initializing REAL DPR-Net (V2) Model...")
    # (Mistral ë‹¤ìš´ë¡œë“œëŠ” ì´ë¯¸ ì™„ë£Œë˜ì—ˆì„ ê²ƒì…ë‹ˆë‹¤)
    
    model = DPR_Net(
        vetnet_dim=cfg_model_vet['dim'],
        vetnet_num_blocks=cfg_model_vet['num_blocks'],
        vetnet_refinement_blocks=cfg_model_vet['refinement_blocks'],
        vetnet_heads=cfg_model_vet['heads'],
        vetnet_ffn_exp=cfg_model_vet['ffn_exp'],
        vetnet_bias=cfg_model_vet['bias'],
        vetnet_ln_type=cfg_model_vet['ln_type'],
        vetnet_volterra_rank=cfg_model_vet['volterra_rank'],
        clip_model_name=cfg_model_clip['model_name'],
        clip_pretrained=cfg_model_clip['pretrained'],
        clip_embed_dim=cfg_model_clip['embed_dim'],
        llm_model_name=cfg_model_llm['model_name'],
        llm_embed_dim=cfg_model_llm['embed_dim']
    ).to(device)
    
    # --- 5. ì†ì‹¤ í•¨ìˆ˜ ì´ˆê¸°í™” ---
    criterion = DPRV2Loss(
        lambda_film=cfg_loss['lambda_film'],
        lambda_consistency=cfg_loss['lambda_consistency'],
        clip_model_name=cfg_model_clip['model_name'],
        clip_pretrained=cfg_model_clip['pretrained'],
        llm_embed_dim=cfg_model_llm['embed_dim']
    ).to(device)

    # --- 6. ì˜µí‹°ë§ˆì´ì € ì„¤ì • ---
    params_to_train = list(filter(lambda p: p.requires_grad, model.parameters()))
    params_to_train.extend(list(criterion.consistency_projector.parameters()))
    
    print(f"\nTotal trainable parameters: {sum(p.numel() for p in params_to_train):,}")
    
    optimizer = AdamW(params_to_train, lr=cfg_train['learning_rate'], weight_decay=1e-4)
    
    # --- [ìˆ˜ì • 3] GradScaler ì´ˆê¸°í™” ---
    scaler = GradScaler()
    
    # --- 7. í•™ìŠµ ë£¨í”„ ---
    print(f"\n--- Starting DPR-Net V2 REAL Training (AMP Enabled) ---")
    for epoch in range(cfg_train['epochs']):
        model.train() 
        criterion.consistency_projector.train()
        
        epoch_loss = 0.0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{cfg_train['epochs']}")
        
        for distorted_img, clean_img in pbar:
            distorted_img = distorted_img.to(device)
            clean_img = clean_img.to(device)
            
            # (ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”)
            optimizer.zero_grad()
            
            # --- [ìˆ˜ì • 4] autocastë¡œ Forward Pass ë˜í•‘ ---
            # (float16ìœ¼ë¡œ ì—°ì‚°)
            with autocast():
                # 1. Forward pass
                outputs = model(distorted_img)
                
                # 2. Calculate V2 Loss
                loss, loss_dict = criterion(
                    outputs, 
                    clean_img, 
                    model.tokenizer, # (DPR_Netì´ ì†Œìœ í•œ) LLM í† í¬ë‚˜ì´ì €
                    device
                )
            
            # --- [ìˆ˜ì • 5] Scalerë¥¼ ì‚¬ìš©í•œ Backward Pass ---
            scaler.scale(loss).backward()
            
            # --- [ìˆ˜ì • 6] Scalerë¥¼ ì‚¬ìš©í•œ Optimizer Step ---
            scaler.step(optimizer)
            
            # --- [ìˆ˜ì • 7] Scaler ì—…ë°ì´íŠ¸ ---
            scaler.update()
            
            epoch_loss += loss.item()
            pbar.set_postfix(
                total=f"{loss_dict['total']:.4f}",
                img=f"{loss_dict['img']:.4f}",
                film=f"{loss_dict['film']:.4f}",
                cons=f"{loss_dict['consistency']:.4f}"
            )
            
        print(f"Epoch {epoch+1} Average Loss: {epoch_loss / len(train_loader):.4f}")
        
        # --- 8. ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ---
        if (epoch + 1) % cfg_train['save_every'] == 0:
            checkpoint_path = os.path.join(cfg_paths['checkpoint_dir'], f"dpr_net_epoch_{epoch+1}.pth")
            
            save_state = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'criterion_state_dict': criterion.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scaler_state_dict': scaler.state_dict(), # --- [ìˆ˜ì • 8] Scaler ìƒíƒœ ì €ì¥
                'config': config # ì„¤ì • íŒŒì¼ë„ í•¨ê»˜ ì €ì¥
            }
            torch.save(save_state, checkpoint_path)
            print(f"Checkpoint saved to {checkpoint_path}")

    print("--- Training Finished ---")


if __name__ == "__main__":
    config_file_path = "configs/dpr_config.yaml" 
    main(config_file_path)