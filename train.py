# G:\DPR-Net\train.py

import os
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from tqdm import tqdm
import open_clip # 'open-clip-torch'

# --- 1. ëª¨ë¸ ì„í¬íŠ¸ ---
# (G:\DPR-Net\model\ í´ë”ì—ì„œ DPR_Netì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤)
# (í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ DummyDPR_Netë„ ì„í¬íŠ¸í•©ë‹ˆë‹¤)
try:
    from model.dpr_net import DPR_Net, DummyDPR_Net
except ImportError:
    print("Error: 'model' íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("G:\\DPR-Net\\model\\__init__.py íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

# -----------------------------------------------
#  â‘¦ V2 Training Loss (L_total)
# -----------------------------------------------

class DPRV2Loss(nn.Module):
    """
    DPR-Net V2ì˜ â‘¦ë²ˆ ì „ì²´ ì†ì‹¤ í•¨ìˆ˜
    L_total = L_img + Î»1 * L_film + Î»2 * L_consistency
    """
    def __init__(self, 
                 lambda_film=0.1, 
                 lambda_consistency=0.01,
                 clip_model_name="ViT-L-14",
                 clip_pretrained="laion2b_s32b_b82k",
                 llm_embed_dim=4096, # Mistral-7B
                 clip_embed_dim=1024 # ViT-L-14
                 ):
        super().__init__()
        print(f"Initializing DPRV2Loss (Î»_film={lambda_film}, Î»_consistency={lambda_consistency})")
        
        self.lambda_film = lambda_film
        self.lambda_consistency = lambda_consistency

        # --- L_img (ë³µì› ì†ì‹¤) ---
        # L1 Lossê°€ ì´ë¯¸ì§€ ë³µì›ì—ì„œ ê°€ì¥ ë³´í¸ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        self.l1_loss = nn.L1Loss()
        
        # --- L_consistency (ì¼ê´€ì„± ì†ì‹¤) ---
        # "|| CLIP_text_embed(text) - hidden_state ||^2"
        
        # 1. MSE (L2) ì†ì‹¤
        self.mse_loss = nn.MSELoss()
        
        # 2. CLIP Text Encoder (L_consistency ê³„ì‚°ìš©)
        print("  Loading CLIP Text Encoder for L_consistency...")
        self.clip_text_model, _, _ = open_clip.create_model_and_transforms(
            clip_model_name, pretrained=clip_pretrained
        )
        self.clip_tokenizer = open_clip.get_tokenizer(clip_model_name)
        
        # CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”ëŠ” ë™ê²° (í•™ìŠµ ëŒ€ìƒ ì•„ë‹˜)
        for param in self.clip_text_model.parameters():
            param.requires_grad = False
        print("  CLIP Text Encoder frozen.")
            
        # 3. Projector (D_llm -> D_clip)
        # LLM hidden_state ì°¨ì›(4096)ì„ CLIP í…ìŠ¤íŠ¸ ì„ë² ë”© ì°¨ì›(1024)ìœ¼ë¡œ ë§¤í•‘
        self.consistency_projector = nn.Linear(llm_embed_dim, clip_embed_dim)
        
        # (ì°¸ê³ ) L_consistencyë¥¼ ìœ„í•œ projectorëŠ” DPRV2Loss ëª¨ë“ˆì´ ì†Œìœ í•˜ë©°
        # ë©”ì¸ optimizerì— ì˜í•´ í•™ìŠµë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

    def calculate_film_loss(self, hidden_state):
        """
        L_film (Stage Alignment) ê³„ì‚°.
        V2 ìŠ¤í™: ê° ìŠ¤í…Œì´ì§€ë¥¼ ì œì–´í•˜ëŠ” í† í°(0~7)ë“¤ì´
        ì„œë¡œ ë‹¤ë¥¸ ì—­í• ì„ í•˜ë„ë¡ ê°•ì œ (Decorrelation)
        """
        # (B, N+1, D_llm)
        num_tokens_to_align = 8 # (token_stage_map ê¸°ì¤€ 0~7)
        if hidden_state.shape[1] < num_tokens_to_align:
            return 0 # ê³„ì‚° ë¶ˆê°€
            
        # (B, 8, D_llm)
        control_tokens = hidden_state[:, :num_tokens_to_align, :]
        
        # L2 ì •ê·œí™”
        tokens_norm = F.normalize(control_tokens, p=2, dim=2)
        
        # (B, 8, D_llm) @ (B, D_llm, 8) -> (B, 8, 8)
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬
        cosine_sim_matrix = torch.bmm(tokens_norm, tokens_norm.transpose(1, 2))
        
        # ìš°ë¦¬ëŠ” 'ë¹„'ëŒ€ê°(off-diagonal) ìš”ì†Œ (í† í° ê°„ ìœ ì‚¬ë„)ê°€
        # 0ì´ ë˜ê¸°ë¥¼ ì›í•©ë‹ˆë‹¤. (ëŒ€ê° ìš”ì†ŒëŠ” í•­ìƒ 1)
        
        # (B, 8, 8)ì—ì„œ ëŒ€ê° ìš”ì†Œë¥¼ 0ìœ¼ë¡œ ë§Œë“¦
        eye = torch.eye(num_tokens_to_align, device=cosine_sim_matrix.device).expand_as(cosine_sim_matrix)
        off_diagonal_sim = cosine_sim_matrix * (1 - eye)
        
        # L_film: ë¹„ëŒ€ê° ìš”ì†Œë“¤ì˜ ì œê³± í‰ê·  (0ì— ê°€ê¹Œì›Œì§€ë„ë¡)
        loss_film = torch.mean(off_diagonal_sim.pow(2))
        return loss_film

    def calculate_consistency_loss(self, hidden_state, logits, llm_tokenizer, device):
        """
        L_consistency (ì„¤ëª…-ì œì–´ ì¼ì¹˜) ê³„ì‚°.
        V2 ìŠ¤í™: || Project(LLM_Global_Token) - CLIP_Embed(Generated_Text) ||^2
        """
        
        # --- 1. Control Path: LLM Global Token (ì œì–´ ì‹ í˜¸) ---
        # (B, N+1, D_llm) -> (B, D_llm)
        llm_global_hidden = hidden_state[:, 0, :] # CLS í† í°
        
        # (B, D_llm) -> (B, D_clip)
        projected_hidden = self.consistency_projector(llm_global_hidden)
        
        
        # --- 2. Text Path: CLIP Embed(Generated_Text) (ì„¤ëª… ì‹ í˜¸) ---
        
        # (B, N+1, Vocab) -> (B, N+1)
        # Logitsì—ì„œ í…ìŠ¤íŠ¸ í† í° ìƒì„± (Greedy decoding)
        # (ì°¸ê³ : ì´ ì—°ì‚°ì€ .detach()ë¡œ ì¸í•´ ì—­ì „íŒŒ ê·¸ë˜í”„ì— í¬í•¨ë˜ì§€ ì•ŠìŒ)
        generated_tokens = torch.argmax(logits.detach(), dim=-1)
        
        # (B, N+1) -> [str_b1, str_b2, ...]
        # ìƒì„±ëœ í† í° IDë¥¼ ì‹¤ì œ í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë””ì½”ë”©
        generated_text_list = llm_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        
        # [str] -> (B, 77)
        # ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸ë¥¼ CLIP í† í¬ë‚˜ì´ì €ë¡œ ë‹¤ì‹œ í† í°í™”
        clip_text_tokens = self.clip_tokenizer(generated_text_list).to(device)
        
        # (B, 77) -> (B, D_clip)
        # CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¡œ ì„ë² ë”©
        with torch.no_grad(): # CLIP ì¸ì½”ë”ëŠ” í•­ìƒ ë™ê²°
            clip_text_embed = self.clip_text_model.encode_text(clip_text_tokens)
            
        # (ì¤‘ìš”) íƒ€ê²Ÿ ì„ë² ë”©ì€ ì—­ì „íŒŒê°€ íë¥´ì§€ ì•Šë„ë¡ detach()
        target_text_embed = clip_text_embed.float().detach()

        
        # --- 3. L2 Loss (MSE) ---
        # || ì œì–´ ì‹ í˜¸ - ì„¤ëª… ì‹ í˜¸ ||^2
        loss_consistency = self.mse_loss(projected_hidden, target_text_embed)
        
        return loss_consistency

    def forward(self, outputs, target_img, llm_tokenizer, device):
        """
        Args:
            outputs (dict): DPR_Netì˜ forward ì¶œë ¥ ë”•ì…”ë„ˆë¦¬
            target_img (torch.Tensor): (B, 3, H, W) ì •ë‹µ(clean) ì´ë¯¸ì§€
            llm_tokenizer: model.tokenizer (í…ìŠ¤íŠ¸ ìƒì„±ìš©)
            device: í˜„ì¬ ì¥ì¹˜
            
        Returns:
            total_loss (torch.Tensor): L_total (ì—­ì „íŒŒìš©)
            loss_dict (dict): ë¡œê¹…ìš© ê°œë³„ ì†ì‹¤
        """
        
        # 1. L_img (ë³µì› ì†ì‹¤)
        l_img = self.l1_loss(outputs['img_restored'], target_img)
        
        # 2. L_film (ìŠ¤í…Œì´ì§€ ì •ë ¬ ì†ì‹¤)
        l_film = self.calculate_film_loss(outputs['hidden_state'])
        
        # 3. L_consistency (ì¼ê´€ì„± ì†ì‹¤)
        l_consistency = self.calculate_consistency_loss(
            outputs['hidden_state'], 
            outputs['logits'], 
            llm_tokenizer,
            device
        )
        
        # 4. L_total
        total_loss = l_img + \
                     self.lambda_film * l_film + \
                     self.lambda_consistency * l_consistency
                     
        loss_dict = {
            'total': total_loss.item(),
            'img': l_img.item(),
            'film': l_film.item(),
            'consistency': l_consistency.item()
        }
        
        return total_loss, loss_dict


# -----------------------------------------------
#  (ì„ì‹œ) ë”ë¯¸ ë°ì´í„°ì…‹ (G:\DPR-Net\data\dataset.py ëŒ€ì²´)
# -----------------------------------------------
class DummyImageDataset(Dataset):
    """ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì„ì˜ì˜ (ì™œê³¡, ì •ë‹µ) ì´ë¯¸ì§€ ìŒ ìƒì„± """
    def __init__(self, num_samples=1000, img_size=(128, 128)):
        self.num_samples = num_samples
        self.img_size = img_size
        print(f"Using DummyImageDataset with {num_samples} random samples.")

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # (0~1 ë²”ìœ„ì˜ ëœë¤ í…ì„œ)
        h, w = self.img_size
        distorted = torch.rand(3, h, w)
        clean = torch.rand(3, h, w)
        return distorted, clean


# -----------------------------------------------
#  ğŸš€ ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (train.py)
# -----------------------------------------------
def main(args):
    # --- 1. ì„¤ì • ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
    os.makedirs(args.checkpoint_dir, exist_ok=True)

    # --- 2. ë°ì´í„° ë¡œë” ---
    # (TODO: G:\DPR-Net\data\dataset.pyì˜ ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ êµì²´ í•„ìš”)
    train_dataset = DummyImageDataset(num_samples=100, img_size=(args.patch_size, args.patch_size))
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=4,
        pin_memory=True
    )
    
    # (TODO: Validation loader ì¶”ê°€)

    # --- 3. ëª¨ë¸ ì´ˆê¸°í™” ---
    # (ì‹¤ì œ í•™ìŠµ ì‹œ)
    # model = DPR_Net(vetnet_dim=48, ...).to(device)
    
    # (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©: Mistral-7B ë‹¤ìš´ë¡œë“œ X)
    print("Initializing DPR-Net in [Dummy Mode] for training test...")
    model_config = {
        'vetnet_dim': 48,
        'vetnet_num_blocks': [2, 2, 2, 2], # (í…ŒìŠ¤íŠ¸ìš© ì¶•ì†Œ)
        'vetnet_refinement_blocks': 2,
        'clip_embed_dim': 1024, # ViT-L-14
        'llm_embed_dim': 4096,  # Mistral-7B
    }
    model = DummyDPR_Net(**model_config).to(device)
    
    # --- 4. ì†ì‹¤ í•¨ìˆ˜ ì´ˆê¸°í™” ---
    criterion = DPRV2Loss(
        lambda_film=args.lambda_film,
        lambda_consistency=args.lambda_consistency,
        llm_embed_dim=model_config['llm_embed_dim'],
        clip_embed_dim=model_config['clip_embed_dim']
    ).to(device)

    # --- 5. ì˜µí‹°ë§ˆì´ì € ì„¤ì • ---
    # V2 ì•„í‚¤í…ì²˜: LLM/CLIP ë³¸ì²´ëŠ” 'ë™ê²°' (frozen)
    # í•™ìŠµ ëŒ€ìƒ: VETNet, FiLMHead, LLM_Input_Projector, Consistency_Projector
    
    # 1. DPR_Net ë‚´ë¶€ì˜ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°
    # (dpr_modules.pyì—ì„œ requires_grad=Trueë¡œ ì„¤ì •í•œ ë ˆì´ì–´ë“¤ + VETNet ì „ì²´)
    params_to_train = list(filter(lambda p: p.requires_grad, model.parameters()))
    
    # 2. DPRV2Loss ë‚´ë¶€ì˜ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° (consistency_projector)
    params_to_train.extend(list(criterion.consistency_projector.parameters()))
    
    print(f"\nTotal trainable parameters: {sum(p.numel() for p in params_to_train):,}")
    
    optimizer = AdamW(params_to_train, lr=args.learning_rate, weight_decay=1e-4)
    
    # (TODO: Learning rate scheduler ì¶”ê°€)

    # --- 6. í•™ìŠµ ë£¨í”„ ---
    print(f"\n--- Starting DPR-Net V2 Training ---")
    for epoch in range(args.epochs):
        model.train() # í•™ìŠµ ëª¨ë“œ
        criterion.consistency_projector.train() # ì†ì‹¤ í•¨ìˆ˜ ë‚´ í”„ë¡œì í„°ë„ í•™ìŠµ ëª¨ë“œ
        
        epoch_loss = 0.0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{args.epochs}")
        
        for distorted_img, clean_img in pbar:
            distorted_img = distorted_img.to(device)
            clean_img = clean_img.to(device)
            
            # 1. Forward pass
            outputs = model(distorted_img)
            
            # 2. Calculate V2 Loss
            loss, loss_dict = criterion(
                outputs, 
                clean_img, 
                model.tokenizer, # (DprLLMì´ ì†Œìœ í•œ) LLM í† í¬ë‚˜ì´ì € ì „ë‹¬
                device
            )
            
            # 3. Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            pbar.set_postfix(
                total=f"{loss_dict['total']:.4f}",
                img=f"{loss_dict['img']:.4f}",
                film=f"{loss_dict['film']:.4f}",
                cons=f"{loss_dict['consistency']:.4f}"
            )
            
        print(f"Epoch {epoch+1} Average Loss: {epoch_loss / len(train_loader):.4f}")
        
        # (TODO: Validation step)

        # --- 7. ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ---
        if (epoch + 1) % args.save_every == 0:
            checkpoint_path = os.path.join(args.checkpoint_dir, f"dpr_net_epoch_{epoch+1}.pth")
            
            # (ëª¨ë¸ + ì†ì‹¤í•¨ìˆ˜ í”„ë¡œì í„° + ì˜µí‹°ë§ˆì´ì € ì €ì¥)
            save_state = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'criterion_state_dict': criterion.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }
            torch.save(save_state, checkpoint_path)
            print(f"Checkpoint saved to {checkpoint_path}")

    print("--- Training Finished ---")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train DPR-Net (V2)")
    
    # ê²½ë¡œ
    parser.add_argument("--data_dir", type=str, default="./data/train", help="Training data directory")
    parser.add_argument("--checkpoint_dir", type=str, default="./checkpoints", help="Directory to save checkpoints")
    
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument("--epochs", type=int, default=100, help="Number of epochs")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size")
    parser.add_argument("--patch_size", type=int, default=128, help="Training patch size (H and W)")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="Learning rate")
    
    # V2 ì†ì‹¤ ëŒë‹¤ (ê°€ì¤‘ì¹˜)
    parser.add_argument("--lambda_film", type=float, default=0.1, help="Weight for L_film (stage alignment)")
    parser.add_argument("--lambda_consistency", type=float, default=0.01, help="Weight for L_consistency (control-text)")
    
    # ê¸°íƒ€
    parser.add_argument("--save_every", type=int, default=5, help="Save checkpoint every N epochs")

    args = parser.parse_args()
    main(args)