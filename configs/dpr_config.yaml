# ==============================================================================
# âš™ï¸ DPR-Net V2 Configuration File
# ==============================================================================

# [í”„ë¡œì íŠ¸ ê¸°ë³¸ ì •ë³´]
project_name: "DPR-Net-V2-PixelLM"
version: "v2.0_mistral_lora"
seed: 42  # ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ ê³ ì •

# [ê²½ë¡œ ì„¤ì •]
paths:
  # ë°ì´í„°ì…‹ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
  root_dir: "G:/DPR-Net/data"
  
  # ì „ì²˜ë¦¬ëœ ìº¡ì…˜ íŒŒì¼ (preprocessing/preprocess_captions.py ì‹¤í–‰ í›„ ìƒì„±ë¨)
  metadata_file: "G:/DPR-Net/data/metadata_captions.json"
  
  # í•™ìŠµì— ì‚¬ìš©í•  í•˜ìœ„ í´ë” ëª©ë¡ (í•„ìš”ì— ë”°ë¼ ì£¼ì„ í•´ì œ/ì¶”ê°€)
  train_folders:
    - "rain100H/train"     # ë¹„ ì œê±°
    - "lol_dataset/our485" # ì €ì¡°ë„ í–¥ìƒ
    - "CSD/train"          # ëˆˆ/êµ¬ë¦„ ì œê±°
    - "SOTS/indoor"        # ì•ˆê°œ ì œê±° (ì˜ˆì‹œ)
  
  # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
  log_dir: "G:/DPR-Net/logs"

# [ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ì •]
model:
  # ğŸ§  Brain: LLM (Mistral-7B)
  llm_model_id: "mistralai/Mistral-7B-v0.1" 
  # ë§Œì•½ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œ ë°›ì•˜ë‹¤ë©´ ë¡œì»¬ ê²½ë¡œë¡œ ë³€ê²½ (ì˜ˆ: "G:/models/Mistral-7B-v0.1")
  
  # ğŸ‘ï¸ Eyes: Vision Encoder (CLIP ViT-Large)
  vision_model_id: "openai/clip-vit-large-patch14"
  
  # ğŸ–ï¸ Hands: Restoration Network (VETNet)
  vetnet_channels: [64, 128, 256, 512] # U-Net ì±„ë„ ê¹Šì´

  # í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê´€ë ¨
  max_length: 128       # ìº¡ì…˜ ìµœëŒ€ í† í° ê¸¸ì´ (ë„ˆë¬´ ê¸¸ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡± ë°œìƒ)
  use_gradient_checkpointing: true # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í•„ìˆ˜ (ì†ë„ëŠ” ì•½ê°„ ëŠë ¤ì§)

# [LoRA (Low-Rank Adaptation) ì„¤ì •]
# ê±°ëŒ€í•œ LLM ì „ì²´ë¥¼ í•™ìŠµí•˜ì§€ ì•Šê³ , ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•˜ì—¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
lora:
  r: 8                  # Rank (í´ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ë§ìŒ, 8~16 ì¶”ì²œ)
  lora_alpha: 32        # Scaling Factor
  lora_dropout: 0.05
  bias: "none"
  target_modules:       # Mistralì˜ ëª¨ë“  ì„ í˜• ë ˆì´ì–´ì— ì ìš©
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# [í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°]
train:
  batch_size: 2         # GPU ë©”ëª¨ë¦¬(24GB ê¸°ì¤€)ì— ë”°ë¼ ì¡°ì ˆ (Mistralì´ ì»¤ì„œ ì‘ê²Œ ì‹œì‘)
  accumulate_grad_batches: 8 # ì‹¤ì œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ íš¨ê³¼ = 2 * 8 = 16
  num_epochs: 10
  learning_rate: 1e-4   # LLM í•™ìŠµ ì‹œ ì¼ë°˜ì ìœ¼ë¡œ ë‚®ì€ LR ì‚¬ìš©
  weight_decay: 0.01
  
  # ì´ë¯¸ì§€ í¬ê¸° ì„¤ì •
  llm_img_size: 224     # CLIP/LLM ì…ë ¥ìš© (ê³ ì •)
  train_img_size: 256   # VETNet í•™ìŠµìš© íŒ¨ì¹˜ í¬ê¸° (Random Crop)

  # ë°ì´í„° ë¡œë” ì„¤ì •
  num_workers: 4        # ë°ì´í„° ë¡œë”© ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜
  persistent_workers: true

# [ì‹œìŠ¤í…œ ì„¤ì •]
system:
  device: "cuda"        # "cuda" or "cpu"
  precision: "bf16-mixed" # "16-mixed" (fp16) or "bf16-mixed" (Ampere GPU ì´ìƒ ì¶”ì²œ)