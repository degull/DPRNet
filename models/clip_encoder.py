# ëˆˆ (Frozen CLIP)
import torch
import torch.nn as nn
from transformers import CLIPVisionModel

# ==============================================================================
# ğŸ‘ï¸ The Eyes: CLIP Vision Encoder
# ì—­í• : ì´ë¯¸ì§€ë¥¼ ë³´ê³  257ê°œì˜ ì˜ë¯¸ë¡ ì  í† í°(CLS + Patches)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
# íŠ¹ì§•: í•™ìŠµë˜ì§€ ì•Šë„ë¡ Frozen ìƒíƒœë¥¼ ìœ ì§€í•˜ì—¬ ê°•ë ¥í•œ íŠ¹ì§• ì¶”ì¶œ ëŠ¥ë ¥ì„ ë³´ì¡´í•©ë‹ˆë‹¤.
# ==============================================================================

class CLIPVisionEncoder(nn.Module):
    def __init__(self, model_id="openai/clip-vit-large-patch14"):
        """
        Args:
            model_id (str): Hugging Face Model ID (ê¸°ë³¸ê°’: ViT-Large/14)
        """
        super().__init__()
        print(f"ğŸ‘ï¸ Loading CLIP Vision Model: {model_id}...")
        
        # CLIPì˜ Vision Encoder ë¶€ë¶„ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
        # (Text EncoderëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
        self.vision_model = CLIPVisionModel.from_pretrained(model_id)
        
        # ğŸ§Š Freeze Parameters (í•™ìŠµ ë°©ì§€)
        # CLIPì€ ì´ë¯¸ì§€ë¥¼ ë³´ëŠ” ë²•ì„ ì´ë¯¸ ì˜ ì•Œê³  ìˆìœ¼ë¯€ë¡œ, 
        # ì´ ê°€ì¤‘ì¹˜ê°€ í•™ìŠµ ì¤‘ì— ë§ê°€ì§€ì§€ ì•Šë„ë¡ ê³ ì •í•©ë‹ˆë‹¤.
        self.vision_model.eval() # í‰ê°€ ëª¨ë“œ ê³ ì •
        for param in self.vision_model.parameters():
            param.requires_grad = False
            
        print("   âœ… CLIP Model Frozen successfully.")

    def forward(self, pixel_values):
        """
        Args:
            pixel_values: [Batch, 3, 224, 224] - ì •ê·œí™”ëœ ì´ë¯¸ì§€ í…ì„œ
            
        Returns:
            image_embeds: [Batch, 257, 1024] - (1 CLS Token + 256 Patch Tokens)
        """
        # CLIP Vision Model Forward Pass
        # output_hidden_states=Trueë¥¼ í•˜ì§€ ì•Šì•„ë„ ê¸°ë³¸ì ìœ¼ë¡œ last_hidden_stateëŠ” ë°˜í™˜ë¨
        outputs = self.vision_model(pixel_values=pixel_values)
        
        # âš ï¸ ì¤‘ìš”: pooler_outputì´ ì•„ë‹Œ last_hidden_stateë¥¼ ì‚¬ìš©í•´ì•¼ í•¨!
        # pooler_output: [Batch, 1024] -> ì´ë¯¸ì§€ë¥¼ 1ê°œì˜ ë²¡í„°ë¡œ ì••ì¶•í•´ë²„ë¦¼ (ê³µê°„ ì •ë³´ ì†Œì‹¤)
        # last_hidden_state: [Batch, 257, 1024] -> íŒ¨ì¹˜ë³„ ì •ë³´ê°€ ì‚´ì•„ìˆìŒ (ë³µì›ì— í•„ìˆ˜)
        return outputs.last_hidden_state