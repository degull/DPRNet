# ğŸ—ï¸ ì „ì²´ ì¡°ë¦½ (Main Model)
# G:\DPR-Net\models\dpr_net_v2.py
print("ğŸ”¥ dpr_net_v2.py LOADED FROM:", __file__)

import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import torch.nn as nn

# ê° ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from models.clip_encoder import CLIPVisionEncoder
from models.mistral_llm import MistralLLM
from models.pixel_decoder import PixelDecoder
from models.film import FiLMGenerator
from models.vetnet import VETNet

# ==============================================================================
# ğŸ—ï¸ DPR-Net V2: The Final Assembly
# "From Auto-Captioning to Spatial Reconstruction"
#
# êµ¬ì¡°:
# 1. Eyes (CLIP)        -> ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (Frozen)
# 2. Brain (Mistral)    -> í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  (LoRA)
# 3. Translator         -> LLM ì‚¬ê³ ë¥¼ ê³µê°„ ë§µìœ¼ë¡œ ë³€í™˜ (PixelDecoder)
# 4. Controller (FiLM)  -> ë³µì› ì œì–´ ì‹ í˜¸(gamma, beta) ìƒì„±
# 5. Hands (VETNet)     -> ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë³µì› ìˆ˜í–‰
# ==============================================================================

class DPRNetV2(nn.Module):
    def __init__(self, config):
        """
        Args:
            config (dict or object): ëª¨ë¸ ì„¤ì •ê°’ (dpr_config.yaml ë‚´ìš©)
        """
        super().__init__()

        # ì„¤ì •ê°’ ë¡œë“œ (Dictionary or Namespace ì²˜ë¦¬)
        if isinstance(config, dict):
            llm_id       = config["model"]["llm_model_id"]
            vision_id    = config["model"]["vision_model_id"]
            vet_channels = config["model"]["vetnet_channels"]
        else:
            llm_id       = config.model.llm_model_id
            vision_id    = config.model.vision_model_id
            vet_channels = config.model.vetnet_channels

        print("\n" + "=" * 60)
        print("ğŸ—ï¸ Initializing DPR-Net V2 (PixelLM Powered)...")
        print("=" * 60)

        # [2ë‹¨ê³„: ëˆˆ] The Eyes (Frozen CLIP)
        self.eyes = CLIPVisionEncoder(model_id=vision_id)

        # [3-1, 3-2ë‹¨ê³„: ë‡Œ] The Brain (Reasoning LLM)
        self.brain = MistralLLM(
            model_id=llm_id,
            vision_hidden_size=1024,  # CLIP-Large Output
            llm_hidden_size=4096,     # Mistral Hidden
        )

        # [3-3ë‹¨ê³„: ë‡Œ] The Translator (Spatial Reconstruction: Pixel Decoder)
        self.pixel_decoder = PixelDecoder(
            input_dim=4096,
            hidden_dim=512,
            output_dim=4096,
        )

        # [4ë‹¨ê³„: ì»¨íŠ¸ë¡¤ëŸ¬] The Controller (FiLM Generator)
        self.controller = FiLMGenerator(
            input_dim=4096,
            vetnet_channels=vet_channels,  # ì˜ˆ: [64, 128, 256, 512]
        )

        # [5ë‹¨ê³„: ì†] The Hands (Restoration Backbone: VETNet)
        self.hands = VETNet(
            in_channels=3,
            out_channels=3,
            dim=vet_channels[0],  # Base dim = 64
            # ë‚˜ë¨¸ì§€ Restormer/VETNet íŒŒë¼ë¯¸í„°ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš© (í•„ìš” ì‹œ configì—ì„œ ì£¼ì…)
        )

        print("âœ… DPR-Net V2 Assembly Complete!\n")

    def forward(self, batch):
        """
        Args:
            batch (dict): DataLoaderì—ì„œ ì˜¬ë¼ì˜¨ ë°°ì¹˜ ë°ì´í„°
                - pixel_values:   [B, 3, 224, 224]          (CLIPìš©)
                - input_ids:      [B, Text_Len]              (Mistralìš©)
                - attention_mask: [B, 257+Text_Len]          (Mistralìš©, Vision+Text)
                - high_res_images:[B, 3, H, W] or List[Tensor] (VETNetìš©)

        Returns:
            restored_image: [B, 3, H, W]
        """
        # 1. ë°ì´í„° ì–¸íŒ¨í‚¹
        clip_imgs = batch["pixel_values"]      # [B, 3, 224, 224]
        text_ids  = batch["input_ids"]         # [B, T]
        attn_mask = batch["attention_mask"]    # [B, 257+T] (ì„¤ê³„ ìƒ)

        high_res = batch["high_res_images"]    # [B, 3, H, W] ë˜ëŠ” List[Tensor]

        # ----------------------------------------------------------------------
        # high_res íƒ€ì… ì²˜ë¦¬ (Tensor or List[Tensor])
        # ----------------------------------------------------------------------
        if isinstance(high_res, list):
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°: í¬ê¸°ê°€ ëª¨ë‘ ê°™ìœ¼ë©´ stack, ì•„ë‹ˆë©´ batch_size=1ë§Œ í—ˆìš©
            if len(high_res) > 0 and isinstance(high_res[0], torch.Tensor):
                try:
                    high_res = torch.stack(high_res).to(clip_imgs.device)
                except RuntimeError:
                    if len(high_res) == 1:
                        high_res = high_res[0].unsqueeze(0).to(clip_imgs.device)
                    else:
                        raise ValueError(
                            "[DPRNetV2] high_res_images have different sizes. "
                            "Use batch_size=1 or pre-crop to a fixed size."
                        )
            else:
                raise ValueError("[DPRNetV2] high_res_images list is empty or not Tensor list.")
        elif isinstance(high_res, torch.Tensor):
            # Tensorì¸ ê²½ìš°: CLIP ì´ë¯¸ì§€ì™€ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ë§ì¶°ì¤Œ
            high_res = high_res.to(clip_imgs.device)
        else:
            raise TypeError("[DPRNetV2] high_res_images must be Tensor or List[Tensor].")

        # ----------------------------------------------------------------------
        # Step 2: Eyes (Vision Encoding)
        # Output: [B, 257, 1024]
        # ----------------------------------------------------------------------
        vision_embeds = self.eyes(clip_imgs)  # (B, 257, 1024)

        # ----------------------------------------------------------------------
        # Step 3: Brain (Reasoning with Mistral)
        # Input:  vision_embeds [B, 257, 1024]
        #         text_ids      [B, T]
        #         attention_mask[B, 257+T]
        # Output: llm_output    [B, 257+T, 4096]
        # ----------------------------------------------------------------------
        B_v, L_v, _ = vision_embeds.shape
        B_t, L_t   = text_ids.shape
        B_m, L_m   = attn_mask.shape

        # ğŸ” Sanity check: ë°°ì¹˜ í¬ê¸°ì™€ mask ê¸¸ì´ ì¼ì¹˜ ì—¬ë¶€
        assert B_v == B_t == B_m, \
            f"[DPRNetV2] Batch size mismatch: vision={B_v}, text={B_t}, mask={B_m}"
        expected_len = L_v + L_t
        assert L_m == expected_len, \
            f"[DPRNetV2] attention_mask length mismatch: expected {expected_len}, got {L_m}"

        llm_output = self.brain(vision_embeds, text_ids, attn_mask)  # (B, 257+T, 4096)

        # ----------------------------------------------------------------------
        # Step 4: Translator (Decoding Thought to Spatial Map)
        # PixelDecoder ë‚´ë¶€ì—ì„œ Vision Part (ì²˜ìŒ 257ê°œ)ë§Œ ì‚¬ìš©í•˜ì—¬
        # [B, 257, 4096] -> [B, 257, 4096] (Refined)
        # ----------------------------------------------------------------------
        spatial_features = self.pixel_decoder(llm_output)  # (B, 257, 4096) ê¸°ëŒ€

        # ----------------------------------------------------------------------
        # Step 5: Controller (Generating FiLM Control Signals)
        # Input:  spatial_features [B, 257, 4096]
        # Output: film_signals: List[(gamma, beta), ...] for each VET stage
        # ----------------------------------------------------------------------
        film_signals = self.controller(spatial_features)

        # ----------------------------------------------------------------------
        # Step 6: Hands (Restoration with Volterra & FiLM)
        # Input:  high_res      [B, 3, H, W]
        #         film_signals  List[(gamma, beta), ...]
        # Output: restored_image [B, 3, H, W]
        # ----------------------------------------------------------------------
        restored_image = self.hands(high_res, film_signals)

        return restored_image

    # ==========================================================================
    # ğŸ—£ï¸ Extra: Chat Mode (For Debugging & Explanation)
    # ì´ë¯¸ì§€ë¥¼ ë³´ê³  LLMì´ ë­ë¼ê³  ìƒê°í–ˆëŠ”ì§€ í…ìŠ¤íŠ¸ë¡œ ë¬¼ì–´ë³´ëŠ” í•¨ìˆ˜
    # ==========================================================================
    def chat_about_image(self, batch, max_new_tokens: int = 50):
        clip_imgs = batch["pixel_values"]
        text_ids  = batch["input_ids"]

        # CLIPìœ¼ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        vision_embeds = self.eyes(clip_imgs)  # (B, 257, 1024)

        # Mistralì—ê²Œ í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­ (ì„¤ëª… ëª¨ë“œ)
        captions = self.brain.generate_caption(
            vision_embeds,
            text_ids,
            max_new_tokens=max_new_tokens,
        )
        return captions
