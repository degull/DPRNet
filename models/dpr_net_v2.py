# ğŸ—ï¸ ì „ì²´ ì¡°ë¦½ (Main Model)
import torch
import torch.nn as nn

# ê° ëª¨ë“ˆ ê°€ì ¸ì˜¤ê¸°
from models.clip_encoder import CLIPVisionEncoder
from models.mistral_llm import MistralLLM
from models.pixel_decoder import PixelDecoder
from models.film_layer import FiLMGenerator
from models.vetnet import VETNet

# ==============================================================================
# ğŸ—ï¸ DPR-Net V2: The Final Assembly
# "From Auto-Captioning to Spatial Reconstruction"
#
# êµ¬ì¡°:
# 1. Eyes (CLIP) -> ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (Frozen)
# 2. Brain (Mistral) -> í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  (LoRA)
# 3. Translator (PixelDecoder) -> LLM ì‚¬ê³ ë¥¼ ê³µê°„ ë§µìœ¼ë¡œ ë³€í™˜
# 4. Controller (FiLM) -> ë³µì› ì œì–´ ì‹ í˜¸(gamma, beta) ìƒì„±
# 5. Hands (VETNet) -> ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë³µì› ìˆ˜í–‰
# ==============================================================================

class DPRNetV2(nn.Module):
    def __init__(self, config):
        """
        Args:
            config (dict or object): ëª¨ë¸ ì„¤ì •ê°’ (dpr_config.yaml ë‚´ìš©)
        """
        super().__init__()
        
        # ì„¤ì •ê°’ ë¡œë“œ (Dictionary or Namespace ì²˜ë¦¬)
        if isinstance(config, dict):
            # dictë¡œ ë“¤ì–´ì˜¬ ê²½ìš° í¸ì˜ë¥¼ ìœ„í•´ ë‚´ë¶€ ë³€ìˆ˜ë¡œ í• ë‹¹
            llm_id = config['model']['llm_model_id']
            vision_id = config['model']['vision_model_id']
            vet_channels = config['model']['vetnet_channels']
        else:
            # Hydra/OmegaConf ë“±ìœ¼ë¡œ ë¡œë“œëœ ê°ì²´ì¼ ê²½ìš°
            llm_id = config.model.llm_model_id
            vision_id = config.model.vision_model_id
            vet_channels = config.model.vetnet_channels

        print("\n" + "="*60)
        print("ğŸ—ï¸ Initializing DPR-Net V2 (PixelLM Powered)...")
        print("="*60)

        # [2ë‹¨ê³„: ëˆˆ] The Eyes
        self.eyes = CLIPVisionEncoder(model_id=vision_id)

        # [3-1, 3-2ë‹¨ê³„: ë‡Œ] The Brain (Reasoning)
        self.brain = MistralLLM(
            model_id=llm_id, 
            vision_hidden_size=1024, # CLIP-Large Output
            llm_hidden_size=4096     # Mistral Hidden
        )

        # [3-3ë‹¨ê³„: ë‡Œ] The Translator (Spatial Reconstruction)
        self.pixel_decoder = PixelDecoder(
            input_dim=4096, 
            hidden_dim=512, 
            output_dim=4096
        )

        # [4ë‹¨ê³„: ì»¨íŠ¸ë¡¤ëŸ¬] The Controller
        self.controller = FiLMGenerator(
            input_dim=4096, 
            vetnet_channels=vet_channels # [64, 128, 256, 512]
        )

        # [5ë‹¨ê³„: ì†] The Hands (Restoration)
        # ì‚¬ìš©ì ì •ì˜ Volterra Layerê°€ í¬í•¨ëœ VETNet
        self.hands = VETNet(
            in_channels=3, 
            out_channels=3, 
            dim=vet_channels[0], # Base dim = 64
            # ë‚˜ë¨¸ì§€ Restormer íŒŒë¼ë¯¸í„°ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš© (í•„ìš” ì‹œ configì—ì„œ ì£¼ì… ê°€ëŠ¥)
        )
        
        print("âœ… DPR-Net V2 Assembly Complete!\n")

    def forward(self, batch):
        """
        Args:
            batch (dict): DataLoaderì—ì„œ ì˜¬ë¼ì˜¨ ë°°ì¹˜ ë°ì´í„°
                - pixel_values: [B, 3, 224, 224] (CLIPìš©)
                - input_ids: [B, Text_Len] (Mistralìš©)
                - attention_mask: [B, 257+Text_Len] (Mistralìš©)
                - high_res_images: [B, 3, H, W] (VETNetìš©)
        
        Returns:
            restored_image: [B, 3, H, W]
        """
        # 1. ë°ì´í„° ì–¸íŒ¨í‚¹
        clip_imgs = batch['pixel_values']
        text_ids = batch['input_ids']
        attn_mask = batch['attention_mask']
        
        # VETNet ì…ë ¥ ì´ë¯¸ì§€ëŠ” List[Tensor]ì¼ ìˆ˜ë„ ìˆê³  Stacked Tensorì¼ ìˆ˜ë„ ìˆìŒ
        # í•™ìŠµ ì‹œì—” Stacked Tensor, ì¶”ë¡  ì‹œì—” Listì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬ í•„ìš”
        high_res = batch['high_res_images']
        if isinstance(high_res, list):
            # ë¦¬ìŠ¤íŠ¸ë¼ë©´(ì¶”ë¡  ì‹œ ì´ë¯¸ì§€ í¬ê¸°ê°€ ë‹¤ë¥´ë©´), ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ 
            # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ stackì´ ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, 1ê°œì”© ì²˜ë¦¬í•´ì•¼ í•¨.
            # * í•™ìŠµ ë¡œë”(dataset.py)ì—ì„œ cropì„ í†µí•´ í¬ê¸°ë¥¼ ë§ì¶°ì„œ stackí•´ì„œ ì£¼ëŠ” ê²ƒì„ ì›ì¹™ìœ¼ë¡œ í•¨.
            if len(high_res) > 0 and isinstance(high_res[0], torch.Tensor):
                 try:
                    high_res = torch.stack(high_res).to(clip_imgs.device)
                 except:
                    # í¬ê¸°ê°€ ë‹¬ë¼ì„œ ìŠ¤íƒ ë¶ˆê°€ ì‹œ, ë°°ì¹˜ ì‚¬ì´ì¦ˆ 1ì¸ ê²½ìš°ë§Œ í—ˆìš©
                    if len(high_res) == 1:
                        high_res = high_res[0].unsqueeze(0).to(clip_imgs.device)
                    else:
                        raise ValueError("Batch processing requires images of same size. Use batch_size=1 for variable sizes.")
        
        # ----------------------------------------------------------------------
        # Step 2: Eyes (Vision Encoding)
        # Output: [B, 257, 1024]
        # ----------------------------------------------------------------------
        vision_embeds = self.eyes(clip_imgs)

        # ----------------------------------------------------------------------
        # Step 3: Brain (Reasoning)
        # Output: [B, 257+N, 4096]
        # ----------------------------------------------------------------------
        llm_output = self.brain(vision_embeds, text_ids, attn_mask)

        # ----------------------------------------------------------------------
        # Step 4: Translator (Decoding Thought to Spatial Map)
        # Output: [B, 257, 4096] (Vision Part Only, Refined)
        # ----------------------------------------------------------------------
        spatial_features = self.pixel_decoder(llm_output)

        # ----------------------------------------------------------------------
        # Step 5: Controller (Generating Control Signals)
        # Output: List of [(gamma, beta), ...]
        # ----------------------------------------------------------------------
        film_signals = self.controller(spatial_features)

        # ----------------------------------------------------------------------
        # Step 6: Hands (Restoration with Volterra & FiLM)
        # Output: [B, 3, H, W]
        # ----------------------------------------------------------------------
        restored_image = self.hands(high_res, film_signals)

        return restored_image

    # ==========================================================================
    # ğŸ—£ï¸ Extra: Chat Mode (For Debugging & Explanation)
    # ì´ë¯¸ì§€ë¥¼ ë³´ê³  LLMì´ ë­ë¼ê³  ìƒê°í–ˆëŠ”ì§€ í…ìŠ¤íŠ¸ë¡œ ë¬¼ì–´ë³´ëŠ” í•¨ìˆ˜
    # ==========================================================================
    def chat_about_image(self, batch, max_new_tokens=50):
        clip_imgs = batch['pixel_values']
        text_ids = batch['input_ids']
        
        # CLIPìœ¼ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        vision_embeds = self.eyes(clip_imgs)
        
        # Mistralì—ê²Œ í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­
        captions = self.brain.generate_caption(
            vision_embeds, 
            text_ids, 
            max_new_tokens=max_new_tokens
        )
        return captions