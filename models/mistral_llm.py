# G:\DPR-Net\models\mistral_llm.py
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training

# ==============================================================================
# ğŸ§  The Brain: Mistral-7B LLM (4-bit Quantized + FP32 Adapters)
# ==============================================================================

class MistralLLM(nn.Module):
    def __init__(
        self,
        model_id: str = "mistralai/Mistral-7B-v0.1",
        vision_hidden_size: int = 1024,
        llm_hidden_size: int = 4096,
    ):
        super().__init__()
        print(f"ğŸ§  Loading Mistral LLM (4-bit): {model_id}...")

        # 1. Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        if self.tokenizer.pad_token is None:
            # Use EOS as PAD (common for causal LMs)
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # 2. 4-bit Quantization Config
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )

        # 3. Load Model in 4-bit
        self.llm = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=bnb_config,
            device_map="auto",
        )

        # í•™ìŠµ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ (LayerNorm ë“±ì„ FP32ë¡œ ë³€í™˜)
        self.llm = prepare_model_for_kbit_training(self.llm)

        # 4. Projector (Adapter): CLIP(1024) -> Mistral(4096)
        # GradScaler í˜¸í™˜ì„±ì„ ìœ„í•´ í•™ìŠµ ê°€ëŠ¥í•œ ë ˆì´ì–´ëŠ” FP32ë¡œ ë‘”ë‹¤.
        self.projector = nn.Linear(vision_hidden_size, llm_hidden_size)
        self.projector.float()

        # 5. Apply LoRA
        print("ğŸš€ Applying LoRA (Low-Rank Adaptation)...")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules=[
                "q_proj",
                "k_proj",
                "v_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj",
            ],
        )
        self.llm = get_peft_model(self.llm, peft_config)
        self.llm.print_trainable_parameters()

        # âœ… ì•ˆì „ì¥ì¹˜: ëª¨ë“  í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ FP32ë¡œ ê°•ì œ ë³€í™˜ (GradScaler ì—ëŸ¬ ë°©ì§€)
        for param in self.parameters():
            if param.requires_grad:
                param.data = param.data.float()

    def forward(self, image_embeds, input_ids, attention_mask):
        """
        image_embeds: (B, 257, vision_hidden_size=1024) from CLIP
        input_ids:    (B, T) text token ids
        attention_mask: (B, 257 + T) where:
            - vision tokens (0..256) = 1
            - text tokens   (257..257+L-1) = 1
            - pad region    (else) = 0
        Returns:
            last_hidden_state: (B, 257 + T, 4096)
        """
        # 1) Vision Projection: 1024 -> 4096
        image_embeds_proj = self.projector(image_embeds)  # (B, 257, 4096)

        # 2) Text Embedding
        text_embeds = self.llm.get_input_embeddings()(input_ids)  # (B, T, 4096)

        # 3) Concatenate along sequence dimension
        inputs_embeds = torch.cat([image_embeds_proj, text_embeds], dim=1)  # (B, 257+T, 4096)

        # ğŸ”¥ Gradient chain ìœ ì§€: PixelLM â†’ Pixel Decoder â†’ FiLM â†’ VETNetê¹Œì§€ end-to-end
        inputs_embeds.requires_grad_(True)

        if inputs_embeds.requires_grad:
            # ë””ë²„ê¹…ìš©: LLM ì…ë ¥ ì„ë² ë”©ì˜ gradientë¥¼ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ retain
            inputs_embeds.retain_grad()

        # 4) Position IDs (0,1,2,..., 257+T-1)
        #    Vision í† í° 257ê°œëŠ” í•­ìƒ 0~256, í…ìŠ¤íŠ¸ëŠ” ê·¸ ì´í›„ë¡œ ì •ë ¬
        seq_len = inputs_embeds.size(1)
        position_ids = torch.arange(seq_len, device=inputs_embeds.device).unsqueeze(0)  # (1, 257+T)

        # 5) Sanity check: attention_mask ê¸¸ì´ê°€ inputs_embedsì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€
        assert (
            attention_mask.shape[1] == seq_len
        ), f"[MistralLLM] attention_mask length mismatch: {attention_mask.shape} vs {inputs_embeds.shape}"

        # 6) Mistral Forward (LoRA í•™ìŠµ)
        outputs = self.llm(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            position_ids=position_ids,
            output_hidden_states=True,
        )

        # 7) Return last hidden state (B, 257+T, 4096)
        return outputs.hidden_states[-1]

    def generate_caption(self, image_embeds, input_ids, max_new_tokens: int = 50):
        """
        ì„¤ëª…/ë””ë²„ê·¸ ìš© ìº¡ì…˜ ìƒì„± ëª¨ë“œ.
        ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Mistral generate() í˜¸ì¶œ.
        """
        # 1) Vision Projection
        image_embeds_proj = self.projector(image_embeds)  # (B, 257, 4096)

        # 2) Text Embedding
        text_embeds = self.llm.get_input_embeddings()(input_ids)  # (B, T, 4096)

        # 3) Concatenate
        inputs_embeds = torch.cat([image_embeds_proj, text_embeds], dim=1)  # (B, 257+T, 4096)

        batch_size, seq_len, _ = inputs_embeds.shape
        vision_len = image_embeds_proj.size(1)
        text_len = text_embeds.size(1)

        # 4) Attention mask: vision=1, text(non-pad)=1, text-pad=0
        text_mask = (input_ids != self.tokenizer.pad_token_id).long()  # (B, T)
        vision_mask = torch.ones(batch_size, vision_len, device=inputs_embeds.device, dtype=torch.long)
        attention_mask = torch.cat([vision_mask, text_mask], dim=1)  # (B, 257+T)

        # 5) Position IDs
        position_ids = torch.arange(seq_len, device=inputs_embeds.device).unsqueeze(0)  # (1, 257+T)

        # 6) Generate text (ì„¤ëª… ëª¨ë“œ)
        outputs = self.llm.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            pad_token_id=self.tokenizer.pad_token_id,
            do_sample=True,
            temperature=0.7,
            position_ids=position_ids,  # model_kwargsë¡œ forwardì— ì „ë‹¬ë¨
        )

        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
