# ë‡Œ (Mistral + LoRA + Text Generation)
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import get_peft_model, LoraConfig, TaskType

# ==============================================================================
# ğŸ§  The Brain: Mistral-7B LLM with LoRA
# ì—­í• : ì‹œê°ì  í† í°ê³¼ í…ìŠ¤íŠ¸ í† í°ì„ ê²°í•©í•˜ì—¬ 'ë©€í‹°ëª¨ë‹¬ ì¶”ë¡ 'ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
# í•µì‹¬: Projector(ì°¨ì› ë³€í™˜) + Concatenation + LoRA + Gradient Stabilization
# ==============================================================================

class MistralLLM(nn.Module):
    def __init__(self, model_id="mistralai/Mistral-7B-v0.1", vision_hidden_size=1024, llm_hidden_size=4096):
        """
        Args:
            model_id: Mistral ëª¨ë¸ ID
            vision_hidden_size: CLIP ì¶œë ¥ ì°¨ì› (ê¸°ë³¸ 1024)
            llm_hidden_size: Mistral ì…ë ¥ ì°¨ì› (ê¸°ë³¸ 4096)
        """
        super().__init__()
        print(f"ğŸ§  Loading Mistral LLM: {model_id}...")
        
        # 1. Tokenizer ë¡œë“œ (íŒ¨ë”© í† í° ì„¤ì • í•„ìˆ˜)
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.tokenizer.pad_token = self.tokenizer.eos_token # Mistralì€ pad_tokenì´ ì—†ìœ¼ë¯€ë¡œ eosë¡œ ëŒ€ì²´
        
        # 2. Mistral ëª¨ë¸ ë¡œë“œ (FP16/BF16 ê¶Œì¥ - ë©”ëª¨ë¦¬ ì ˆì•½)
        # device_map="auto"ë¥¼ í†µí•´ ê°€ëŠ¥í•œ ê²½ìš° GPUì— ìë™ í• ë‹¹
        self.llm = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"
        )
        
        # 3. Projector (The Adapter)
        # CLIPì˜ 1024ì°¨ì› ë²¡í„°ë¥¼ Mistralì˜ 4096ì°¨ì› ê³µê°„ìœ¼ë¡œ ì˜ì•„ì˜¬ë¦¬ëŠ” ì„ í˜• ë³€í™˜
        self.projector = nn.Linear(vision_hidden_size, llm_hidden_size)
        
        # 4. LoRA (Low-Rank Adaptation) ì„¤ì •
        # ê±°ëŒ€ ëª¨ë¸ ì „ì²´ë¥¼ í•™ìŠµí•˜ëŠ” ê±´ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, ì¼ë¶€ íŒŒë¼ë¯¸í„°(LoRA Adapter)ë§Œ í•™ìŠµ
        print("ğŸš€ Applying LoRA (Low-Rank Adaptation)...")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False, 
            r=8,            # Rank (ë†’ì„ìˆ˜ë¡ í‘œí˜„ë ¥ ì¦ê°€, ë©”ëª¨ë¦¬ ì¦ê°€)
            lora_alpha=32,  # Scaling Factor
            lora_dropout=0.05,
            # Mistralì˜ ëª¨ë“  Attention Linear Layer íƒ€ê²ŸíŒ…
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        )
        self.llm = get_peft_model(self.llm, peft_config)
        self.llm.print_trainable_parameters() # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ë¹„ìœ¨ ì¶œë ¥

    def forward(self, image_embeds, input_ids, attention_mask):
        """
        Args:
            image_embeds: [Batch, 257, 1024] (from CLIP)
            input_ids: [Batch, Text_Len] (Text Tokens)
            attention_mask: [Batch, 257 + Text_Len] (Combined Mask)
            
        Returns:
            last_hidden_state: [Batch, 257 + Text_Len, 4096]
        """
        # 1. Vision Embedding Projection
        # [B, 257, 1024] -> [B, 257, 4096]
        image_embeds_proj = self.projector(image_embeds)
        
        # 2. Text Embedding Extraction
        # Mistral ë‚´ë¶€ì˜ Embedding Layerë¥¼ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ IDë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        # [B, Text_Len] -> [B, Text_Len, 4096]
        text_embeds = self.llm.get_input_embeddings()(input_ids)
        
        # 3. Concatenation (ì´ì–´ ë¶™ì´ê¸°)
        # [Vision Tokens] + [Text Tokens]
        inputs_embeds = torch.cat([image_embeds_proj, text_embeds], dim=1)
        
        # âš¡ 4. Gradient Checkpointing Stabilization (ëª…ì„¸ì„œ í•µì‹¬)
        # ì´ í…ì„œì— grad ì¶”ì ì„ ê°•ì œí•˜ì—¬, Projectorì™€ LLM ì‚¬ì´ì˜ Backprop ë‹¨ì ˆì„ ë°©ì§€í•©ë‹ˆë‹¤.
        # (íŠ¹íˆ LLM ë³¸ì²´ê°€ Frozen ìƒíƒœì´ê±°ë‚˜ LoRAë¥¼ ì“¸ ë•Œ ì…ë ¥ë‹¨ Gradê°€ ëŠê¸°ëŠ” ë¬¸ì œ í•´ê²°)
        if inputs_embeds.requires_grad:
            inputs_embeds.retain_grad()
        
        # 5. Mistral Forward Pass
        # input_ids ëŒ€ì‹  inputs_embedsë¥¼ ì§ì ‘ ë„£ì–´ì¤ë‹ˆë‹¤.
        outputs = self.llm(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            output_hidden_states=True # Hidden State ë°˜í™˜ í™œì„±í™”
        )
        
        # ì „ì²´ Hidden State ë°˜í™˜
        # (Slicingì€ Pixel Decoder í˜¹ì€ Main Modelì—ì„œ ìˆ˜í–‰)
        return outputs.hidden_states[-1]

    # ==========================================================================
    # ğŸ—£ï¸ Optional: Text Generation Mode (for Debugging/Captioning)
    # ëª…ì„¸ì„œì˜ "ì„¤ëª… ëª¨ë“œ" êµ¬í˜„ì„ ìœ„í•œ í•¨ìˆ˜
    # ==========================================================================
    def generate_caption(self, image_embeds, input_ids, max_new_tokens=50):
        """
        ì´ë¯¸ì§€ë¥¼ ë³´ê³  LLMì´ ìƒê°í•˜ëŠ” ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
        """
        image_embeds_proj = self.projector(image_embeds)
        text_embeds = self.llm.get_input_embeddings()(input_ids)
        inputs_embeds = torch.cat([image_embeds_proj, text_embeds], dim=1)
        
        # generate() í•¨ìˆ˜ëŠ” inputs_embeds ì…ë ¥ì„ ì§€ì›í•©ë‹ˆë‹¤.
        outputs = self.llm.generate(
            inputs_embeds=inputs_embeds,
            max_new_tokens=max_new_tokens,
            pad_token_id=self.tokenizer.pad_token_id,
            do_sample=True, # ì°½ì˜ì ì¸ ìƒì„±ì„ ìœ„í•´ ìƒ˜í”Œë§ ì‚¬ìš©
            temperature=0.7
        )
        
        # ìƒì„±ëœ í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)