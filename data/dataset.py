# ==============================================================================
# ðŸ“‹ DPR-Net V2 Dataset Loader
# ì—­í• : ì´ë¯¸ì§€ì™€ ìº¡ì…˜ì„ ë¡œë“œí•˜ê³ , LLMê³¼ Vision Encoderì— ë§žê²Œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
# í•µì‹¬: ê°€ë³€ ê¸¸ì´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° Attention Mask 2D ì •ë ¬ (Vision + Text)
# ==============================================================================

import os
import json
import torch
from torch.utils.data import Dataset
from PIL import Image
from transformers import AutoTokenizer, CLIPImageProcessor
import torchvision.transforms as transforms

class DPRDataset(Dataset):
    def __init__(self, data_root, metadata_file, tokenizer_path="mistralai/Mistral-7B-v0.1", img_size=224, max_length=128):
        self.data_root = data_root
        self.img_size = img_size
        self.max_length = max_length
        
        # Metadata ë¡œë“œ
        with open(metadata_file, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        
        self.image_paths = list(self.metadata.keys())
        
        # Tokenizer & Processor
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        self.tokenizer.pad_token = self.tokenizer.eos_token 
        self.clip_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-large-patch14")
        
        # VETNetìš© ì „ì²˜ë¦¬ (í•™ìŠµì„ ìœ„í•´ 256x256 Resizeë¡œ í†µì¼)
        self.transform_vet = transforms.Compose([
            #transforms.Resize(256),
            transforms.RandomCrop(128),
            transforms.ToTensor() 
        ])

    def _get_gt_path(self, input_path):
        """
        ìž…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë‹µ(Ground Truth) ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.
        """
        gt_path = input_path
        
        # 1. Rain100H (rain -> norain)
        if 'rain' in input_path and 'norain' not in input_path:
            gt_path = input_path.replace('rain', 'norain')
        
        # 2. LOL Dataset (low -> high)
        elif 'low' in input_path:
            gt_path = input_path.replace('low', 'high')
            
        # 3. CSD (Snow -> Gt)
        elif 'Snow' in input_path:
            gt_path = input_path.replace('Snow', 'Gt')
            
        # 4. SOTS (hazy -> clear)
        elif 'hazy' in input_path:
            gt_path = input_path.replace('hazy', 'clear')
            
        # íŒŒì¼ í™•ì¸
        if not os.path.exists(gt_path):
            # GTê°€ ì—†ìœ¼ë©´ ìžê¸° ìžì‹ ì„ ë°˜í™˜ (Self-Supervised ëŒ€ë¹„ í˜¹ì€ ì—ëŸ¬ ë°©ì§€)
            # print(f"âš ï¸ Warning: GT not found for {input_path}")
            return input_path
            
        return gt_path

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        caption = self.metadata[img_path]
        
        # GT ê²½ë¡œ ì°¾ê¸°
        gt_path = self._get_gt_path(img_path)
        
        try:
            image = Image.open(img_path).convert('RGB')
            gt_image = Image.open(gt_path).convert('RGB')
        except:
            # ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ì´ë¯¸ì§€ë¡œ ë„˜ì–´ê°
            return self.__getitem__((idx + 1) % len(self))

        # 1. CLIP Input (Vision Encoderìš©)
        pixel_values = self.clip_processor(images=image, return_tensors="pt").pixel_values.squeeze(0)

        # 2. VETNet Input & Target (ë³µì› ë„¤íŠ¸ì›Œí¬ìš©)
        vet_input = self.transform_vet(image)   # ì†ìƒëœ ì´ë¯¸ì§€ (Input)
        vet_target = self.transform_vet(gt_image) # ì •ë‹µ ì´ë¯¸ì§€ (Target)
        
        # 3. Text Input (Mistralìš©)
        text_inputs = self.tokenizer(
            caption, return_tensors="pt", padding="max_length",
            truncation=True, max_length=self.max_length, add_special_tokens=True
        )
        input_ids = text_inputs.input_ids.squeeze(0)
        text_mask = text_inputs.attention_mask.squeeze(0)

        return {
            "pixel_values": pixel_values,
            "input_ids": input_ids,
            "text_mask": text_mask,
            "vet_input": vet_input,
            "vet_target": vet_target
        }

    @staticmethod
    def collate_fn(batch):
        # ë°°ì¹˜ ë°ì´í„° ìŠ¤íƒ
        pixel_values = torch.stack([item['pixel_values'] for item in batch])
        input_ids = torch.stack([item['input_ids'] for item in batch])
        text_mask = torch.stack([item['text_mask'] for item in batch])
        
        # VETNet ë°ì´í„° ìŠ¤íƒ
        vet_input = torch.stack([item['vet_input'] for item in batch])
        vet_target = torch.stack([item['vet_target'] for item in batch])
        
        batch_size = len(batch)
        
        # Vision Mask ìƒì„± (ëª¨ë‘ 1)
        vision_mask = torch.ones((batch_size, 257), dtype=torch.long)
        
        # ì „ì²´ Attention Mask ê²°í•© [Vision | Text]
        attention_mask = torch.cat([vision_mask, text_mask], dim=1)
        
        return {
            "pixel_values": pixel_values,      # [B, 3, 224, 224]
            "input_ids": input_ids,            # [B, Text_Len]
            "attention_mask": attention_mask,  # [B, 257+Text_Len]
            "vet_input": vet_input,            # [B, 3, 256, 256] -> ëª¨ë¸ ìž…ë ¥
            "vet_target": vet_target           # [B, 3, 256, 256] -> Loss ê³„ì‚°ìš© ì •ë‹µ
        }